{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Background Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- preprocess text derived from HTML documents\n",
    "- extract corpus linguistic features\n",
    "- Built a supervised classification system to identify language background of English language learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an end-to-end classification system using a corpus of HTML documents scraped from the [Lang-8 language learning website](https://lang-8.com/). In particular, it is a classifier to distinguish English text written by Lang-8 users whose native language (L1) is another European language (French and Spanish) from those written by L1 speakers of East Asian languages (Japanese, Korean, and Mandarin Chinese). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text extraction from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three things to be extracted from the HTML files inside the ZIP file:\n",
    "- the native language of the writer\n",
    "- the raw text (string) of the entry, with HTML removed\n",
    "- the original filename (so that you can use our provided train/dev/test split) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_zip = ZipFile(\"data/raw/lang-8.zip\")\n",
    "file_names = my_zip.namelist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L1(html):\n",
    "    '''Extract the native language from the html string'''\n",
    "    for language in html.find_all(title='Native language'):\n",
    "        native_language = language.text\n",
    "        return native_language\n",
    "    \n",
    "def get_text(html):\n",
    "    '''Extract the entry text from the html string'''\n",
    "    # \"div\" shoud have \"id\" with \"body_show_ori\"\n",
    "    sent = []\n",
    "    for text in html.find_all(id='body_show_ori'):\n",
    "        for node in text:\n",
    "            if node in text.find_all('br'):\n",
    "                sent.append('\\n')\n",
    "            else:\n",
    "                sent.append(node.text)\n",
    "\n",
    "    text_message = ''.join(sent)\n",
    "    return text_message\n",
    "\n",
    "def get_filename_in_txt(html):\n",
    "    '''Extract the filename from the html string'''\n",
    "    for title in html.find_all(\"title\"):\n",
    "        file_name = title.text.split('|')[0].strip()\n",
    "        return file_name\n",
    "        \n",
    "def get_author_name(html):\n",
    "    '''Return a string as the author of the text'''\n",
    "    for box in html.find_all(id = \"author_box\"):\n",
    "        for names in box.find_all('a', {\"class\": \"user_name\"}):\n",
    "            user_name = names.text\n",
    "            return user_name\n",
    "\n",
    "def get_generator(file_list):\n",
    "    '''Return a dataframe of all extracted information'''\n",
    "    for i in file_list:\n",
    "        file_title = i.split('/')[1]\n",
    "        f = my_zip.open(i)\n",
    "        text_file = f.read()\n",
    "        soup = BeautifulSoup(text_file)\n",
    "        file_name = get_filename_in_txt(soup)\n",
    "        native_lang = get_L1(soup)\n",
    "        author_name = get_author_name(soup)\n",
    "        text_mess = get_text(soup)\n",
    "        f.close()\n",
    "        final_output = {'filetitle': file_title,\n",
    "         'filename': file_name,\n",
    "         'text': text_mess,\n",
    "         'language': native_lang,\n",
    "         'author': author_name\n",
    "        }\n",
    "        yield final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_generator(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filetitle': '100060.html',\n",
       " 'filename': 'In it days of pascuas, we are celebrated',\n",
       " 'text': '\\nIn it days of pascuas, we are celebrated :\\n \\nThe December 21 that is the day of the Spirit of the Christmas, where the angel Gabriel celebrates the arrival to Maria. We celebrate it with it has dinner from friends and family, in this 21 desires are asked for the prime year and one writes a leaf bad what of the year that then burns to eliminate it of our lives.\\n\\nDecember 24 is the day that god celebrates the Christmas or the birth in of the child; in the mangers the figure of the child is placed after 12 of the night as sign of birth. \\n\\nDecember 28 is the Day of the Santos Inocentes, where jokes all are played, this date according to the Catholic Church is one pagan - religious celebrates since masses and festivals realize in commemoration of millions of children salted and dried 2009 years ago with the birth of the Messiah.\\n\\nDecember 31 is the typical festivity with the fireworks, the count is realized before the new year. In we have the tradition of 12 grapes of the time, this tradition it is necessary to eat 12 grapes before 12 a.m. and for every grape that commas you can a desire, that is to say, that have 12 desires in whole.\\n\\nMy favorite taboo 31 with the growing up that if wearing yellow underwear have prosperity in the year, as is if after the 12 salts with a suitcase in the street and greet neighbors and give them the new year have many trips that year, and also the belief of thinking that if you have dollars and the passport in the year you will have prosperity in the year.\\n',\n",
       " 'language': 'Spanish',\n",
       " 'author': 'Riona Rose'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function tests\n",
    "file_name_1 = 'lang-8/167307.html'\n",
    "file_name_2 = 'lang-8/259663.html'\n",
    "\n",
    "test_text = \"\"\"\n",
    "I go to the library and I read the many book.\n",
    "\n",
    "Originally I rarely read the book, but I will read the book \n",
    "\n",
    "in this vacation.\n",
    "\n",
    "This vacation is my last vacation, \n",
    "\n",
    "so I don't want to regret to this vacation's lazy behaviors\n",
    "\n",
    "And I go to nursery school for volunteer in 6p.m.\n",
    "\n",
    "They are very very very cute.  <--Is this correct??:)\n",
    "\n",
    "Anyway This site is good because I met good friend yesterday.\n",
    "\n",
    "She is very kind, so I am impressive. \n",
    "\n",
    "Thanks. Have a good day.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test_text_2 = \"\"\"\n",
    "If I had a time machine, I would travel to the past and try to see how the culture began, how the humans were created. Also I would like to visit the ancient civilizations like the Aztecs, Mayas and Chinese. \n",
    "\n",
    "Bye\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n"
     ]
    }
   ],
   "source": [
    "f = my_zip.open(file_name_1)\n",
    "text_file = f.read()\n",
    "soup = BeautifulSoup(text_file)\n",
    "f.close()\n",
    "assert get_L1(soup) == 'Korean'\n",
    "\n",
    "assert get_text(soup) == test_text\n",
    "\n",
    "assert get_author_name(soup) == 'oldergoku'\n",
    "\n",
    "assert get_filename_in_txt(soup) == \"\"\"Today's planning\"\"\"\n",
    "\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n"
     ]
    }
   ],
   "source": [
    "f = my_zip.open(file_name_2)\n",
    "text_file = f.read()\n",
    "soup = BeautifulSoup(text_file)\n",
    "f.close()\n",
    "assert get_L1(soup) == 'Spanish'\n",
    "\n",
    "assert get_text(soup) == test_text_2\n",
    "\n",
    "assert get_author_name(soup) == 'armacora'\n",
    "\n",
    "assert get_filename_in_txt(soup) == 'A ride in the time machine'\n",
    "\n",
    "print('success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extraction_1 = {'filetitle': '105992.html',\n",
    " 'filename': 'Let me introduce myself',\n",
    " 'text': \"\\nHi!  My name is Alex (Sasha in Russian) and I'm from Belarus.\\nI emigrated to the United states almost five years ago and I live in Brooklyn, NY.\\nUnfortunately I have no one to speak with and have no abilities to practice my English. This is very sad - to live in English speaking country and\\xa0have no way to speak and learn language that is all around you. :(\\nWhen I first came to the United States I didn't know a single word in English.  I remember standing in the McDonalds the first day I came trying to order a French-fries and diet coke - I opened the mouth and... nothing came from it. I was standing and mumbling something. that was horrible.\\nSo, I watched TV and read books - this is the way I learned English.\\nFrom now on I will write here some thoughts about everything what bothers me. About emigration and cultural differences, about feelings, about America and my view on it.\\n\\nCan you please help me to correct my mistakes? Not only grammatical flaws but also the way I speak. Cause the language is not only the words and grammar rules, it is also THE WAY you speak, the way you express your thoughts.  Many people speaking foreign language often speak\\xa0it in a very strange way - the way that natives never speak. Because different languages have different structures and different 'language tools' to express same things.  I hope you understand what I'm talking about.\\n\\nThank you.\\n\",\n",
    " 'language': 'Russian',\n",
    " 'author': 'Sampler'}\n",
    "\n",
    "file_extraction_2 = {'filetitle': '248228.html',\n",
    " 'filename': 'Hello',\n",
    " 'text': \"\\nHello everyone, mi name is Fidel i'm new here, i hope you help me with the enclish i'll help you with the spanish. Thanks\\n\",\n",
    " 'language': 'Spanish',\n",
    " 'author': 'Fidelzch'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n"
     ]
    }
   ],
   "source": [
    "assert next(get_generator(['lang-8/105992.html'])) == file_extraction_1\n",
    "assert next(get_generator(['lang-8/248228.html'])) == file_extraction_2\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "# !{sys.executable} -m install spacy\n",
    "# !{sys.executable} -m spacy download en_core_web_sm\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of file dict for data spliting\n",
    "\n",
    "file_names = [\"data/data-split/train.txt\", \"data/data-split/test.txt\", \"data/data-split/dev.txt\"]\n",
    "file_sets = defaultdict(set)\n",
    "def get_file_set(file_name):\n",
    "    '''Create list of file dict for data spliting'''\n",
    "    with open(file_name) as f:\n",
    "        files = f.readlines()\n",
    "    file_set = set()\n",
    "    for file in files:\n",
    "        file_set.add(file.strip(\"\\n\"))\n",
    "    file_sets[file_name] = file_set\n",
    "    return file_sets\n",
    "\n",
    "for file_name in file_names:\n",
    "    filename_dict = get_file_set(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n"
     ]
    }
   ],
   "source": [
    "# Data splitting\n",
    "\n",
    "train_list = []\n",
    "test_list = []\n",
    "dev_list = []\n",
    "for result in results:\n",
    "    if result['language'] != 'Russian':\n",
    "        if result['filetitle'] in filename_dict['data/data-split/train.txt']:\n",
    "            train_list.append(result)\n",
    "        elif result['filetitle'] in filename_dict['data/data-split/test.txt']:\n",
    "            test_list.append(result)\n",
    "        elif result['filetitle'] in filename_dict['data/data-split/dev.txt']:\n",
    "            dev_list.append(result)\n",
    "\n",
    "assert len(train_list) == 742\n",
    "assert len(test_list) == 252\n",
    "assert len(dev_list) == 246\n",
    "print(\"success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the target \"language\" into European or Asian\n",
    "def get_target(list_of_dict):\n",
    "    '''transform all the \"language\" into European or Asian'''\n",
    "    for dictionary in list_of_dict:\n",
    "        language = dictionary['language']\n",
    "        if language in ['Spanish', 'French']:\n",
    "            dictionary['language'] = language.replace(language, 'European')\n",
    "        elif language in ['Japanese', 'Korean', 'Mandarin']:\n",
    "            dictionary['language'] = language.replace(language, 'Asian')\n",
    "    return list_of_dict\n",
    "\n",
    "train_list = get_target(train_list)\n",
    "test_list = get_target(test_list)\n",
    "dev_list = get_target(dev_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Set-up function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up: function for getting word list from single string\n",
    "def word_segmentation(text):\n",
    "    '''word segmentation and get rid of punctuation and space\n",
    "    Parameters\n",
    "    ----------\n",
    "    test: str\n",
    "        the text that you want to calculate TTR\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of segmented word (dtype: spacy.tokens) without punctuation and space\n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    word_list = []\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.pos_ not in ['PUNCT', 'SPACE']:\n",
    "                word_list.append(word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Feature extraction functions\n",
    "1. `has_asian_text`\n",
    "2. `has_european_text`\n",
    "3. `get_asian_to_european_count_ratio`\n",
    "4. `get_type_token_ratio`\n",
    "5. `get_lemmatized_percentage`\n",
    "6. `get_lexical_density`\n",
    "7. `average_verb_length`\n",
    "8. `average_noun_length`\n",
    "9. `get_readability_score`\n",
    "10. `get_percentage_punctuations_with_no_spaces_after`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Asian and European name sets\n",
    "\n",
    "# Step 1: split train list into Asian and European\n",
    "Asian_train = []\n",
    "European_train = []\n",
    "for train in train_list:\n",
    "    if train['language'] == 'Asian':\n",
    "        Asian_train.append(train)\n",
    "    elif train['language'] == 'European':\n",
    "        European_train.append(train)\n",
    "\n",
    "assert len(Asian_train) + len(European_train) == len(train_list)\n",
    "\n",
    "# Step 2: get Asian and European word sets\n",
    "def get_word_set(list_of_dict):\n",
    "    '''Get word set from list of dictionary'''\n",
    "    words = []\n",
    "    for data in list_of_dict:\n",
    "        word_list = word_segmentation(data['text'])\n",
    "        for word in word_list:\n",
    "            words.append(word)\n",
    "    lemmatized_asian = [word.lemma_ for word in words]\n",
    "    return set(word.lower() for word in lemmatized_asian)\n",
    "\n",
    "asian_set = get_word_set(Asian_train)\n",
    "european_set = get_word_set(European_train)\n",
    "\n",
    "# Step 3: get Asian and European unique word sets\n",
    "#common_set = asian_set & european_set\n",
    "asian_name = asian_set #- common_set\n",
    "europe_name = european_set #- common_set\n",
    "\n",
    "# Append these to the names of places&people for asia and Europe\n",
    "with open(\"data/names/asian.txt\", \"a\", encoding=\"utf-8\") as fout:\n",
    "    for word in asian_name:\n",
    "        fout.write(word+'\\n')\n",
    "\n",
    "with open(\"data/names/european.txt\", \"a\", encoding=\"utf-8\") as fout:\n",
    "    for word in europe_name:\n",
    "        fout.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data, load the sets, and check the ratio of counts\n",
    "file_names = [\"asian.txt\", \"european.txt\"]\n",
    "lex_sets = defaultdict(set)\n",
    "def load_lexicons_set(file_name):\n",
    "    '''Load set of words for each target'''\n",
    "    with open(\"data/names/\" + file_name) as f:\n",
    "        files = f.readlines()\n",
    "    file_set = set()\n",
    "    for file in files:\n",
    "        file_set.add(file.strip(\"\\n\"))\n",
    "    lex_sets[file_name] = file_set\n",
    "    return lex_sets\n",
    "\n",
    "\n",
    "for file_name in file_names:\n",
    "    filename_dict = load_lexicons_set(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_asian_text(text):\n",
    "    '''calculate if Asian names appear in the text exclusively\n",
    "    Parameters\n",
    "    ----------\n",
    "    test: str\n",
    "        the text that you want to calculate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        number of Asian names in the text\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> has_asian_text(train_list[2]['text'])\n",
    "    True\n",
    "    >>> has_asian_text(train_list[60]['text'])\n",
    "    False\n",
    "    '''\n",
    "    word_list = word_segmentation(text)\n",
    "    lemmatized_list = [word.lemma_ for word in word_list]\n",
    "    lemma_set = set(word.lower() for word in lemmatized_list)\n",
    "    return len(lemma_set & (lex_sets['asian.txt']- (lex_sets['asian.txt'] & lex_sets['european.txt']))) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert has_asian_text(train_list[2]['text']) == True  # Asian example must have > 0 Asian name\n",
    "assert has_asian_text(train_list[60]['text']) == False  # European example cannot have European name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_european_text(text):\n",
    "    '''calculate if European names appear in the text\n",
    "    Parameters\n",
    "    ----------\n",
    "    test: str\n",
    "        the text that you want to calculate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        number of European names in the text\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> has_european_text(train_list[2]['text'])\n",
    "    True\n",
    "    >>> has_european_text(train_list[60]['text'])\n",
    "    18\n",
    "    '''\n",
    "    word_list = word_segmentation(text)\n",
    "    lemmatized_list = [word.lemma_ for word in word_list]\n",
    "    lemma_set = set(word.lower() for word in lemmatized_list)\n",
    "    return len(lemma_set & (lex_sets['european.txt']- (lex_sets['asian.txt'] & lex_sets['european.txt']))) >0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert has_european_text(train_list[2]['text']) == False  # Asian example must have 0 European name count\n",
    "assert has_european_text(train_list[60]['text']) == True  # European example must have > 1 European name count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asian_to_european_count_ratio(text):\n",
    "    '''calculate how many Asian names appear in the text compared to European\n",
    "    Parameters\n",
    "    ----------\n",
    "    test: str\n",
    "        the text that you want to calculate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        ratio of Asian names to European names in the text\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> get_asian_to_european_count_ratio(train_list[2]['text'])\n",
    "    7\n",
    "    >>> get_asian_to_european_count_ratio(train_list[60]['text'])\n",
    "    0\n",
    "    '''\n",
    "    word_list = word_segmentation(text)\n",
    "    lemmatized_list = [word.lemma_ for word in word_list]\n",
    "    lemma_set = set(word.lower() for word in lemmatized_list)\n",
    "    euro_len = len(lemma_set & lex_sets['european.txt'])\n",
    "    asia_len = len(lemma_set & lex_sets['asian.txt'])\n",
    "\n",
    "    if euro_len == 0:\n",
    "        return 0\n",
    "    return asia_len/euro_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_asian_to_european_count_ratio(train_list[2]['text']) > 0  # Asian example must have > 1 Asian name count\n",
    "assert get_asian_to_european_count_ratio(train_list[60]['text']) == 0.8363636363636363  # European example must have 0 Asian name count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_token_ratio(text):\n",
    "    '''calculate type-token ratio (TTR) from single text\n",
    "    Parameters\n",
    "    ----------\n",
    "    test: str\n",
    "        the text that you want to calculate TTR\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        type-token ratio\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> get_type_token_ratio(train_list[2]['text'])\n",
    "    0.7323943661971831\n",
    "    >>> get_type_token_ratio(train_list[5]['text'])\n",
    "    0.7843137254901961\n",
    "    '''\n",
    "    word_list = word_segmentation(text)\n",
    "    type_set = set(str(word).lower() for word in word_list)\n",
    "    return len(type_set) / len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_type_token_ratio(train_list[2]['text']) == 0.7323943661971831\n",
    "assert get_type_token_ratio(train_list[5]['text']) == 0.7843137254901961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent of word changed after lemmatization\n",
    "def get_lemmatized_percentage(text):\n",
    "    '''calculate the percentage of tokens that are lemmatized\n",
    "    Parameters\n",
    "    ----------\n",
    "    test: str\n",
    "        the text that you want to calculate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        percentage of lemmatized tokens\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> get_lemmatized_percentage(train_list[2]['text'])\n",
    "    0.057692307692307696\n",
    "    >>> get_lemmatized_percentage(train_list[10]['text'])\n",
    "    0.031746031746031744\n",
    "    '''\n",
    "    word_list = word_segmentation(text)\n",
    "    token_set = set(str(word).lower() for word in word_list)\n",
    "    lemmatized_list = [word.lemma_ for word in word_list]\n",
    "    lemma_set = set(word.lower() for word in lemmatized_list)\n",
    "    return (len(token_set) - len(lemma_set)) / len(token_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_lemmatized_percentage(train_list[2]['text']) == 0.057692307692307696\n",
    "assert get_lemmatized_percentage(train_list[10]['text']) == 0.031746031746031744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexical density\n",
    "def get_lexical_density(text):\n",
    "    '''calculate lexical density\n",
    "    Parameters\n",
    "    ----------\n",
    "    test: str\n",
    "        the text that you want to calculate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        lexical density\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> get_lexical_density(train_list[2]['text'])\n",
    "    0.6056338028169014\n",
    "    >>> get_lexical_density(train_list[10]['text'])\n",
    "    0.5217391304347826\n",
    "    '''\n",
    "    open_class_prefix = {\"N\", \"V\", \"J\", \"R\"}\n",
    "    word_list = word_segmentation(text)\n",
    "    total_open_word = 0\n",
    "    word_list = []\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.tag_[0] in open_class_prefix:\n",
    "                total_open_word += 1\n",
    "            if word.pos_ not in ['PUNCT', 'SPACE']:\n",
    "                word_list.append(str(word))\n",
    "    return total_open_word / len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_lexical_density(train_list[2]['text']) == 0.6056338028169014\n",
    "assert get_lexical_density(train_list[10]['text']) == 0.5217391304347826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_verb_length(text):\n",
    "    '''given a string of text, \n",
    "    tag POS and \n",
    "    then return avg verb len\n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    total_len = 0\n",
    "    num_verbs = 0.01\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.tag_[0] == 'V':\n",
    "                total_len += len(token)\n",
    "                num_verbs += 1\n",
    "                \n",
    "    return round(total_len/num_verbs , 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert average_verb_length(train_list[2]['text']) == 4.37\n",
    "assert average_verb_length(train_list[11]['text']) == 3.96\n",
    "assert average_verb_length('') == 0.0\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_noun_length(text):\n",
    "    '''given a string of text, \n",
    "    tag POS and \n",
    "    then return avg noun len\n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    total_len = 0\n",
    "    num_nouns = 0.01\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.tag_[0] == 'N':\n",
    "                total_len += len(token)\n",
    "                num_nouns += 1\n",
    "                \n",
    "    return round(total_len/num_nouns , 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert average_noun_length(train_list[2]['text']) == 5.66\n",
    "assert average_noun_length(train_list[11]['text']) == 9.9\n",
    "assert average_noun_length('') == 0.0\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate functions\n",
    "from nltk.corpus import cmudict\n",
    "vowels = {\"a\",\"e\",\"i\",\"o\",\"u\",\"y\"}\n",
    "p_dict = cmudict.dict() # keep this outside as a global variable so you aren't reloading each time\n",
    "\n",
    "def get_syllables(word):\n",
    "    \"\"\"Returns the count of syllables in a word.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word : str\n",
    "        word to check the syllables for.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    output : int\n",
    "        The count of syllables in a word\n",
    "    \"\"\"\n",
    "    '''use CMU dict (p_dict) to count the number of syllables in word, default to number of vowels'''\n",
    "    syllable_count = 0\n",
    "    if word not in p_dict.keys():\n",
    "        word = word.lower()\n",
    "        for i in range(len(word)):\n",
    "            if word[i] in vowels:\n",
    "                syllable_count += 1\n",
    "    else:            \n",
    "        for ARPAbet_phone in p_dict[word][0]:\n",
    "            if ARPAbet_phone[-1] in ['0', '1', '2']:\n",
    "                syllable_count += 1  \n",
    "    return syllable_count\n",
    "\n",
    "def get_reading_ease(sentence):\n",
    "    \"\"\"Returns the readability result of formula based on the words in a sentence\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        sentence to check the readability for.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    output : float\n",
    "        The readability score of a sentence\n",
    "    \"\"\"\n",
    "    '''calculate the Flesh reading ease for a single sentence consisting of a list of words (strings)'''\n",
    "    # your code here\n",
    "    count_words = 0\n",
    "    count_syllables = 0\n",
    "    reading_ease = None\n",
    "    for word in sentence:\n",
    "        if word.isalpha():\n",
    "            count_words +=1\n",
    "            count_syllables += get_syllables(word)\n",
    "    if count_words > 0 :\n",
    "        reading_ease = (206.835 - (1.015 * (count_words/1)) - (84.6 * (count_syllables/count_words)))\n",
    "    \n",
    "    return reading_ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readability_score(text):\n",
    "    '''given a string of text, \n",
    "    split it into sentences, \n",
    "    calculate the readability\n",
    "    '''\n",
    "    score = 0\n",
    "    num_sentences = 0\n",
    "    for sent in sent_tokenize(text):\n",
    "        if get_reading_ease(sent):\n",
    "            score += get_reading_ease(sent)\n",
    "            num_sentences += 1\n",
    "            \n",
    "    if num_sentences == 0:\n",
    "        return score\n",
    "                \n",
    "    return round(score/num_sentences , 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert get_readability_score(train_list[2]['text']) == 83.38\n",
    "assert get_readability_score(train_list[11]['text']) == 95.21\n",
    "assert get_readability_score('') == 0.0\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_punctuations_with_no_spaces_after(text):\n",
    "    '''given a string of text, \n",
    "    check the percentage of punctuations which \n",
    "    do no have a space or a newline after\n",
    "    '''\n",
    "    punctuations = {'.', ',', ';'}\n",
    "    spaces = {' ','\\n'}\n",
    "    total_punctuations = 0\n",
    "    count_with_no_spaces = 0\n",
    "    doc = nlp(text)\n",
    "    counter = 0\n",
    "    len_doc = len(text)\n",
    "    for letter in text:\n",
    "        if letter in punctuations:\n",
    "            total_punctuations += 1\n",
    "            if (counter+1) < len_doc and text[counter+1] not in spaces:\n",
    "                count_with_no_spaces += 1\n",
    "\n",
    "        counter += 1\n",
    "      \n",
    "    if total_punctuations == 0:\n",
    "        return 100.0\n",
    "                \n",
    "    return (round(count_with_no_spaces*100/total_punctuations, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "text = '''Chelsea English School is offering a Summer School Program in Iwaki, Fukushima, a holiday learning experience combining enjoyment of the area's natural beauty and practical lifestyle immersion in the agricultural traditions of this part of Japan.\n",
    "We will be hosted by \"Namakiba\" farm, an agricultural concern run by an Iwaki City cooperative, and activities include handson experience of organic farming,barbecues, local nature sightseeing including swimming in the river and the sea,the local fish market, guesthouses with onsens (hot spas) . The program promises new and fresh experiences in both nature and culture, and time will also be made available for gift shopping. Non-Japanese speakers are also warmly invited, as simultaneous translation into English will be available throughout the e trip. '''\n",
    "\n",
    "assert get_percentage_punctuations_with_no_spaces_after(text) == 14.29\n",
    "assert get_percentage_punctuations_with_no_spaces_after(train_list[2]['text']) == 28.57\n",
    "assert get_percentage_punctuations_with_no_spaces_after(train_list[11]['text']) == 100.0\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Building feature dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(list_of_dict):\n",
    "    for data in list_of_dict:\n",
    "        data['has_asian_name'] = has_asian_text(data['text'])\n",
    "        data['has_european_name'] = has_european_text(data['text'])\n",
    "        data['asian_european_ratio'] = get_asian_to_european_count_ratio(data['text'])\n",
    "        data['ttr'] = get_type_token_ratio(data['text'])\n",
    "        data['perc_lemmatized'] = get_lemmatized_percentage(data['text'])\n",
    "        data['lexical_density'] = get_lexical_density(data['text'])\n",
    "        data['avrg_v_length'] = average_verb_length(data['text'])\n",
    "        data['avrg_n_length'] = average_noun_length(data['text'])\n",
    "        data['readability'] = get_readability_score(data['text'])\n",
    "        data['perc_punc_no_space'] = get_percentage_punctuations_with_no_spaces_after(data['text'])\n",
    "    return list_of_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = feature_extraction(train_list)\n",
    "test_list = feature_extraction(test_list)\n",
    "dev_list = feature_extraction(dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filetitle': '100282.html',\n",
       " 'filename': 'Hello, everyone!',\n",
       " 'text': \"\\n\\nMy name is Rikopin. I'm a university student in Japan.\\nI begin to write my journal from today.\\nI'd like you to check it when you are free :-)\\n\\nOne of my favorit singers is Shouta Shimizu.\\nHis new CD was released yesterday!\\nI haven't bought it yet because I have little money(;_;)\\nAnyway, his new song is really nice! \\nI recommend you to listen to this song!\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       " 'language': 'Asian',\n",
       " 'author': 'りこぴん',\n",
       " 'has_asian_name': True,\n",
       " 'has_european_name': False,\n",
       " 'asian_european_ratio': 1.1666666666666667,\n",
       " 'ttr': 0.7323943661971831,\n",
       " 'perc_lemmatized': 0.057692307692307696,\n",
       " 'lexical_density': 0.6056338028169014,\n",
       " 'avrg_v_length': 4.37,\n",
       " 'avrg_n_length': 5.66,\n",
       " 'readability': 83.38,\n",
       " 'perc_punc_no_space': 28.57}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filetitle': '100216.html',\n",
       " 'filename': 'First diary in English',\n",
       " 'text': '\\nHi there,\\n\\nI am Japanese living in Tokyo.\\nSometimes I read English documents for my job, but there were few chances to write in English. I want to learn here.\\n\\nI like listening to / making (especially electronic) music.\\nRecently I made a techno tune for the free online compilation album planned by a friend of mine. It will be soon released!\\n\\nBy the way, it is really cold today. It is about time to get my \"kotatsu\" (Japanese table with an electric heater) ready.\\n',\n",
       " 'language': 'Asian',\n",
       " 'author': 'Go',\n",
       " 'has_asian_name': True,\n",
       " 'has_european_name': True,\n",
       " 'asian_european_ratio': 1.0,\n",
       " 'ttr': 0.7764705882352941,\n",
       " 'perc_lemmatized': 0.06060606060606061,\n",
       " 'lexical_density': 0.611764705882353,\n",
       " 'avrg_v_length': 4.53,\n",
       " 'avrg_n_length': 5.68,\n",
       " 'readability': 74.09,\n",
       " 'perc_punc_no_space': 0.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filetitle': '102725.html',\n",
       " 'filename': 'Recent My Game Life.',\n",
       " 'text': \"\\nI almost finished Assassins Creed 2 Xbox360.\\nI got all 66 view points but I didn't get Xbox360 Gamers Point.\\nI searched it on the internet.\\nI knew Assassins Creed 2 don't have it to collect all view points.\\n\\nDay before yesterday I played Modern Warware 2 Xbox360.\\nI didn't skillfully play it.\\nI don't like FPS but I like it because I enough to feel real.\\n\\nToday I will get Final Fantasy 13 PS3.\\nI want to play it as soon as possible.\\n\",\n",
       " 'language': 'Asian',\n",
       " 'author': 'motionbros',\n",
       " 'has_asian_name': False,\n",
       " 'has_european_name': False,\n",
       " 'asian_european_ratio': 1.0,\n",
       " 'ttr': 0.5909090909090909,\n",
       " 'perc_lemmatized': 0.07692307692307693,\n",
       " 'lexical_density': 0.6022727272727273,\n",
       " 'avrg_v_length': 4.21,\n",
       " 'avrg_n_length': 5.91,\n",
       " 'readability': 88.06,\n",
       " 'perc_punc_no_space': 0.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "- First, vectorize the lists of feature dictionaries appropriately, turning them into first into sparse matrices, and then numpy arrays.\n",
    "- Build a sklearn `DecisionTreeClassifier` with `max_depth=3` (for regularization), and get a preliminary accuracy_score on the _dev_ set using all features.\n",
    "- The next step is feature [ablation](https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence%29), which involves removing features one at a time from the model, and seeing what the effect is.\n",
    "- The last step is to check the accuracy of the model on the _test_ set, after removing any features that are not useful based on _dev_ set results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U scikit-learn\n",
    "from sklearn import tree, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = tree.DecisionTreeClassifier(max_depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(train_list).drop(columns = ['filetitle', 'filename', 'text', 'author', 'language'])\n",
    "y_train = pd.DataFrame(train_list)['language']\n",
    "\n",
    "X_dev = pd.DataFrame(dev_list).drop(columns = ['filetitle', 'filename', 'text', 'author', 'language'])\n",
    "y_dev = pd.DataFrame(dev_list)['language']\n",
    "\n",
    "X_test = pd.DataFrame(test_list).drop(columns = ['filetitle', 'filename', 'text', 'author', 'language'])\n",
    "y_test = pd.DataFrame(test_list)['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation(model, X_train, y_train, X_dev, y_dev, feature):\n",
    "    \"\"\"Removes a feature in the data and finds the accuracy of the model without that feature\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : \n",
    "        the classification model being used.\n",
    "    X_train : \n",
    "        the training data.\n",
    "    y_train : \n",
    "        the training target values.\n",
    "    X_dev :\n",
    "        the development data.\n",
    "    y_dev : \n",
    "        the development target values.\n",
    "    feature : str\n",
    "        the name of the feature to remove\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The accuracy of the model with the missing feature.\n",
    "    \"\"\"\n",
    "    ab_train = X_train.drop(columns = feature)\n",
    "    ab_dev = X_dev.drop(columns = feature)\n",
    "    \n",
    "    model.fit(ab_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(ab_dev)\n",
    "    \n",
    "    return metrics.accuracy_score(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablated_accuracy(model, X_train, y_train, X_dev, y_dev):\n",
    "    \"\"\"Finds the accuracy of the model with each feature missing\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : \n",
    "        the classification model being used.\n",
    "    X_train : \n",
    "        the training data.\n",
    "    y_train : \n",
    "        the training target values.\n",
    "    X_dev :\n",
    "        the development data.\n",
    "    y_dev : \n",
    "        the development target values.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Data Frame\n",
    "        The accuracy of the model with each feature removed.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_dev)\n",
    "    \n",
    "    original_score = metrics.accuracy_score(y_dev, y_pred)\n",
    "    results.append(['none', original_score, 0])\n",
    "    \n",
    "    for column in X_train.columns:\n",
    "        ab_score = ablation(model, X_train, y_train, X_dev, y_dev, column)\n",
    "        results.append([column, ab_score, original_score - ab_score])\n",
    "    \n",
    "    return pd.DataFrame(columns = ['Feature removed', 'Accuracy', 'Difference'], data = results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Feature removed  Accuracy  Difference\n",
      "0                   none  0.711382    0.000000\n",
      "1         has_asian_name  0.776423   -0.065041\n",
      "2      has_european_name  0.703252    0.008130\n",
      "3   asian_european_ratio  0.686992    0.024390\n",
      "4                    ttr  0.711382    0.000000\n",
      "5        perc_lemmatized  0.707317    0.004065\n",
      "6        lexical_density  0.703252    0.008130\n",
      "7          avrg_v_length  0.711382    0.000000\n",
      "8          avrg_n_length  0.699187    0.012195\n",
      "9            readability  0.711382    0.000000\n",
      "10    perc_punc_no_space  0.707317    0.004065\n"
     ]
    }
   ],
   "source": [
    "feat_acc = ablated_accuracy(tree_clf, X_train, y_train, X_dev, y_dev)\n",
    "print(feat_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7764227642276422"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing features that cause the accuracy to go down.\n",
    "for feat, dif in zip(feat_acc['Feature removed'], feat_acc['Difference']):\n",
    "    if dif < 0:\n",
    "        X_train = X_train.drop(columns = feat)\n",
    "        X_dev = X_dev.drop(columns = feat)\n",
    "        X_test = X_test.drop(columns = feat)\n",
    "\n",
    "#Checking new accuracy on train set.        \n",
    "tree_clf.fit(X_train, y_train)\n",
    "    \n",
    "y_pred = tree_clf.predict(X_dev)\n",
    "    \n",
    "metrics.accuracy_score(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(167.4, 190.26, 'X[1] <= 0.992\\ngini = 0.434\\nsamples = 742\\nvalue = [506, 236]'),\n",
       " Text(83.7, 135.9, 'X[0] <= 0.5\\ngini = 0.076\\nsamples = 228\\nvalue = [9, 219]'),\n",
       " Text(41.85, 81.53999999999999, 'gini = 0.0\\nsamples = 9\\nvalue = [9, 0]'),\n",
       " Text(125.55000000000001, 81.53999999999999, 'gini = 0.0\\nsamples = 219\\nvalue = [0, 219]'),\n",
       " Text(251.10000000000002, 135.9, 'X[1] <= 1.008\\ngini = 0.064\\nsamples = 514\\nvalue = [497, 17]'),\n",
       " Text(209.25, 81.53999999999999, 'X[4] <= 0.503\\ngini = 0.478\\nsamples = 43\\nvalue = [26, 17]'),\n",
       " Text(167.4, 27.180000000000007, 'gini = 0.355\\nsamples = 13\\nvalue = [3, 10]'),\n",
       " Text(251.10000000000002, 27.180000000000007, 'gini = 0.358\\nsamples = 30\\nvalue = [23, 7]'),\n",
       " Text(292.95, 81.53999999999999, 'gini = 0.0\\nsamples = 471\\nvalue = [471, 0]')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree.plot_tree(tree_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape[1] <= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "    \n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
